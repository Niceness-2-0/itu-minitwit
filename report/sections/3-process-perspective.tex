\section{Process' perspective}
\begin{comment}
{

## 2. Process' perspective
This perspective should clarify how code or other artifacts come from idea into the running system and everything that happens on the way.

In particular, the following descriptions should be included:



## 2.1 A complete description of stages and tools included in the CI/CD chains, including deployment and release of your systems.

## 2.2 How do you monitor your systems and what precisely do you monitor?(alexandra :/)

## 2.3 What do you log in your systems and how do you aggregate logs? (oscar is such a stud)

## 2.4 Brief results of the security assessment and brief description of how did you harden the security of your system based on the analysis.

## 2.5 Applied strategy for scaling and upgrades.

In case you have used AI-assistants during your project briefly explain which system(s) you used during the project and reflect how it supported or hindered your process.
}
\end{comment}

\subsection{CI/CD pipeline}
% A complete description of stages and tools included in the CI/CD chains, including deployment and release of your systems.
Our CI/CD pipeline was built using GitHub Actions and followed a build, test, deployment, and release structure. The pipeline was automatically triggered on every push to the \texttt{main} branch and could also be run manually via the GitHub interface.

We chose GitHub Actions because it offers a seamless integration with our GitHub repository, requires no external CI/CD tooling, and provides a flexible YAML-based syntax that allowed us to define complex workflows with minimal setup. Additionally, GitHub Actions supports secure secret management, reusable actions, and tight permission control and managing sensitive credentials in our workflow.

In the \textbf{build stage}, Docker images for the API, web frontend, and Promtail are built with Docker Buildx and pushed to Docker Hub, using caching to speed up builds.

During the \textbf{test stage}, we provision a test server over SSH, deploy the latest images, and run API (pytest) and UI (selenium) tests to catch regressions early.

The \textbf{deploy stage} connects to the production server and runs a script that checks for updated images and redeploys the stack using \texttt{docker stack deploy}.

We also added a \textbf{release stage}, where we created a new Git tag and published a release on GitHub, using the version stored in a tracked \texttt{VERSION} file.

\subsection{Monitoring} \label{sec:monitoring}
% 2.2 How do you monitor your systems and what precisely do you monitor?
% Whitebox Monitoring: Internal monitoring — collects app metrics like request count, latency, errors. Requires code instrumentation.
% Blackbox Monitoring: External checks — probes the service from the outside (e.g. HTTP ping) to see if it's up and responsive.
% Pull-Based Monitoring: Prometheus pulls metrics from app endpoints (like /metrics) at regular intervals.
For monitoring, we chose Prometheus\footnote{\url{https://prometheus.io/}} and Grafana\footnote{\url{https://grafana.com/}} for their complementary strengths. Prometheus handles pull-based whitebox metric collection, Grafana provides intuitive dashboards for visualization, and the Blackbox Exporter enables external uptime checks for effective blackbox monitoring. All are open-source, free, and widely adopted technologies with extensive documentation and libraries suited to our Go codebase. They are easy to integrate, making them a great fit for our project and providing a complete system overview.
\\

We used a Prometheus client\footnote{\url{https://github.com/prometheus/client_golang}} in our Go application, exposing metrics via the dedicated \textbf{/metrics} endpoint. Prometheus scrapes this endpoint, collecting data such as request counts, response status codes, and endpoint-specific performance metrics. Blackbox Exporter performs external HTTP probes to verify service availability and responsiveness from the outside.

Grafana is used to visualize these metrics using a custom dashboard, enabling us to track both system-level behavior and user interaction. We monitor functional metrics such as route-specific request volumes, status code, and latency, but we try to gain business-relevant data insights, like total registrations, messages posted. Using this setup we have visibility on the system’s health and usage, allowing us to detect anomalies early and assess the system’s effectiveness in meeting user needs.

\subsection{Logging}
%What do you log in your systems and how do you aggregate logs? (oscar is such a stud)
To aggregate logs across our infrastructure, we decided to use Promtail and Loki. We set up the Promtail service to run in a container, where it checks for updates on any logs files inside a volume that our API writes to. This is then sent to our central logging/monitoring server, where Loki ingests and stores the data. The logs can then be accessed and visualized through Grafana, which is hosted on the same server.
\\

The logs are structured by the help of the Go Library Logrus \footnote{\url{https://github.com/sirupsen/logrus}}, for easy parsing. The API logs: All database errors relating to storing, fetching or updating, and Invalid http requests.
\\

The reason for using Promtail and Loki for log aggregation was that the ELK stack required too many resources. While trying to deploy the ELK stack to one of our VMs, it slowed down the whole server because it used 100\% of the CPU when ElasticSerach was initializing. We then got recommended by another team to use Promtail and Loki, which proved to be great for our infrastructure, as Loki seamlessly integrates into Grafana, which we already used for monitoring.


\subsection{Security assessment}
We conducted a security assessment focusing on key components of our system, including the API, web application, infrastructure, and CI/CD pipeline. After the security assessment we figured out some problems and solved most of them. The following table outlines identified risks and the corresponding mitigation strategies we implemented or planned to implement (Red ones show they are still planned to be implemented):

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|>{\centering\arraybackslash}p{3.5cm}|>{\centering\arraybackslash}p{6cm}|>{\centering\arraybackslash}p{6cm}|}
\hline
\textbf{Asset/Area} & \textbf{Identified Risk} & \textbf{Mitigation Strategy} \\
\hline
API Endpoints & SQL Injection via unsanitized inputs & Input validation and parameterized queries \\
\hline
\color{red}Login System & \color{red} Brute-force attacks on authentication & \color{red} Rate limiting middleware on login endpoints \\
\hline
\color{red} Docker Containers & \color{red} Privilege escalation within containers & \color{red} Containers run as non-root users \\
\hline
Secrets Management & Exposure of sensitive credentials & GitHub Secrets and environment variables used \\
\hline
SSH Access & Unauthorized server access & SSH key authentication; password login disabled \\
\hline
CI/CD Pipeline & Leakage of secrets during deployment & Encrypted GitHub Actions secrets; secure SSH deploy \\
\hline
Logging Mechanism & Sensitive data exposure in logs & Log filtering to exclude confidential data \\
\hline
External Traffic & Eavesdropping / MITM attacks & TLS termination via Cloudflare; HTTPS enforced through NGINX \\
\hline
\end{tabular}
\caption{Summary of Security Risks and Mitigations}
\end{table}



\subsection{Scaling}
%2.5 Applied strategy for scaling and upgrades.
In this project, most of our scaling and upgrades happened in our CI/CD pipeline. After the simulator was started, we needed a way to deploy code without having downtime. In the start, we adopted a blue/green deployment strategy, this was made possible by setting up a reverse proxy\footnote{\url{https://nginx.org/}} for the incoming requests. When deploying new code, a new container was spawned with the changes, which we then redirected the requests to. This provided virtually zero-downtime, as it was only Ngnix's configuration that needed to be reloaded\footnote{\url{https://immersedincode.io.vn/blog/zero-downtime-deployment-with-docker-compose-nginx/}}.
\\

Further along in the project, we adopted a rolling update strategy using Docker Swarm. Our CI/CD pipeline uploaded new Docker images to Docker Hub, and a deployment script on the server checked for image updates. If changes were detected, docker stack deploy was triggered, which allowed Swarm to update services one task at a time. This ensured zero-downtime deployments by gradually replacing containers while keeping the application available. The reason for choosing docker swarm, is because it's better suited for the small application we have. We looked into how to set up Kubernetes, but we agreed that it would be an overkill, and would probably take too long to implement.
\\

Initially, we used Vagrant to provision droplets for our Docker Swarm cluster and test servers. Later in the project, we transitioned to Terraform to automate infrastructure setup. We successfully automated droplet creation and connecting the Swarm cluster. While we didn’t fully automate starting the application, it highlighted Terraform’s power and potential in managing Infrastructure as Code(IaC). We opted for \textbf{Terraform}\footnote{\url{https://developer.hashicorp.com/terraform}}, an open-source tool by HashiCorp, because it allows us to define and manage infrastructure using a simple, declarative language. It works across different cloud providers, making it highly flexible. This helps keep setups consistent, easy to reproduce, and much easier to share or collaborate on.

\subsection{AI assistant} 

The project do not contain any AI assistants in the pipeline. But we have made use of LLMs throughout the project for different tasks. They have been a great resource to help understand, and help with various tasks. Especially in the beginning in the refactor stage, where the team had limited knowledge of Go. The AI assistants were able to explain and produce code that we weren't familiar with in the start.